{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e930d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "os.makedirs('outputs/figures', exist_ok=True)\n",
    "os.makedirs('outputs/results', exist_ok=True)\n",
    "os.makedirs('outputs/gridsearch', exist_ok=True)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"✓ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4ca86",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4889d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (32561, 15)\n",
      "\n",
      "Target distribution:\n",
      "income\n",
      "<=50K    24720\n",
      ">50K      7841\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values: 0\n",
      "\n",
      "✓ Data preprocessed\n",
      "Training set: (22792, 14)\n",
      "Test set: (9769, 14)\n",
      "Class distribution - Train: [17303  5489]\n",
      "Class distribution - Test: [7417 2352]\n",
      "\n",
      "✓ Data preprocessed\n",
      "Training set: (22792, 14)\n",
      "Test set: (9769, 14)\n",
      "Class distribution - Train: [17303  5489]\n",
      "Class distribution - Test: [7417 2352]\n"
     ]
    }
   ],
   "source": [
    "# Load census data\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "           'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "df = pd.read_csv('census/adult.data', names=columns, na_values=' ?', skipinitialspace=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['income'].value_counts())\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode target variable (Binary: 0 = <=50K, 1 = >50K)\n",
    "df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n✓ Data preprocessed\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class distribution - Train: {np.bincount(y_train)}\")\n",
    "print(f\"Class distribution - Test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b12ae5",
   "metadata": {},
   "source": [
    "## 2. Define Hyperparameter Grids for All 6 Classifiers\n",
    "\n",
    "We'll test multiple values for each key hyperparameter to analyze sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613e9e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameter grids defined\n",
      "\n",
      "Total classifiers: 6\n",
      "KNN: 3 hyperparameters, 28 combinations\n",
      "Logistic Regression: 4 hyperparameters, 24 combinations\n",
      "SVM: 3 hyperparameters, 60 combinations\n",
      "MLP: 5 hyperparameters, 60 combinations\n",
      "Decision Tree: 4 hyperparameters, 224 combinations\n",
      "Naive Bayes: 1 hyperparameters, 7 combinations\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grids for sensitivity analysis\n",
    "param_grids = {\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11, 15, 21],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'max_iter': [1000]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]\n",
    "    },\n",
    "    'MLP': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 100), (150, 100, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'max_iter': [500]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Base classifiers\n",
    "classifiers = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'MLP': MLPClassifier(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(\"✓ Hyperparameter grids defined\")\n",
    "print(f\"\\nTotal classifiers: {len(classifiers)}\")\n",
    "for name, grid in param_grids.items():\n",
    "    total_combinations = np.prod([len(v) for v in grid.values()])\n",
    "    print(f\"{name}: {len(grid)} hyperparameters, {total_combinations} combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079767e",
   "metadata": {},
   "source": [
    "## 3. Perform GridSearchCV for All Classifiers\n",
    "\n",
    "This will test all hyperparameter combinations and find the best configuration for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219e9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CENSUS INCOME - HYPERPARAMETER SENSITIVITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GridSearchCV: KNN\n",
      "================================================================================\n",
      "Best CV Score: 0.8369\n",
      "Test Accuracy: 0.8426\n",
      "Test F1-Score: 0.6433\n",
      "\n",
      "Hyperparameter Sensitivity:\n",
      "  Variance: 0.000035\n",
      "  Std Dev: 0.005879\n",
      "  Range: 0.0228\n",
      "\n",
      "Best Parameters:\n",
      "  metric: manhattan\n",
      "  n_neighbors: 21\n",
      "  weights: uniform\n",
      "\n",
      "================================================================================\n",
      "GridSearchCV: Logistic Regression\n",
      "================================================================================\n",
      "Best CV Score: 0.8243\n",
      "Test Accuracy: 0.8258\n",
      "Test F1-Score: 0.5509\n",
      "\n",
      "Hyperparameter Sensitivity:\n",
      "  Variance: 0.000141\n",
      "  Std Dev: 0.011868\n",
      "  Range: 0.0488\n",
      "\n",
      "Best Parameters:\n",
      "  C: 0.01\n",
      "  max_iter: 1000\n",
      "  penalty: l1\n",
      "  solver: liblinear\n",
      "\n",
      "================================================================================\n",
      "GridSearchCV: SVM\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Perform GridSearchCV\u001b[39;00m\n\u001b[32m     17\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     18\u001b[39m     clf, \n\u001b[32m     19\u001b[39m     param_grid, \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     verbose=\u001b[32m0\u001b[39m\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[32m     29\u001b[39m grid_results[name] = {\n\u001b[32m     30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbest_params\u001b[39m\u001b[33m'\u001b[39m: grid_search.best_params_,\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbest_score\u001b[39m\u001b[33m'\u001b[39m: grid_search.best_score_,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mstd_scores\u001b[39m\u001b[33m'\u001b[39m: grid_search.cv_results_[\u001b[33m'\u001b[39m\u001b[33mstd_test_score\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     35\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/principles_of_ml/venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/principles_of_ml/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/principles_of_ml/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/principles_of_ml/venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/principles_of_ml/venv/lib/python3.13/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/principles_of_ml/venv/lib/python3.13/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/principles_of_ml/venv/lib/python3.13/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/principles_of_ml/venv/lib/python3.13/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "grid_results = {}\n",
    "best_models = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CENSUS INCOME - HYPERPARAMETER SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name in classifiers.keys():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GridSearchCV: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    clf = classifiers[name]\n",
    "    param_grid = param_grids[name]\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        clf, \n",
    "        param_grid, \n",
    "        cv=3, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Store results\n",
    "    grid_results[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'cv_results': grid_search.cv_results_,\n",
    "        'all_scores': grid_search.cv_results_['mean_test_score'],\n",
    "        'std_scores': grid_search.cv_results_['std_test_score']\n",
    "    }\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Test on test set\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test_scaled)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate sensitivity metrics\n",
    "    score_variance = np.var(grid_search.cv_results_['mean_test_score'])\n",
    "    score_std = np.std(grid_search.cv_results_['mean_test_score'])\n",
    "    score_range = np.max(grid_search.cv_results_['mean_test_score']) - np.min(grid_search.cv_results_['mean_test_score'])\n",
    "    \n",
    "    grid_results[name]['test_accuracy'] = test_accuracy\n",
    "    grid_results[name]['test_f1'] = test_f1\n",
    "    grid_results[name]['variance'] = score_variance\n",
    "    grid_results[name]['std'] = score_std\n",
    "    grid_results[name]['range'] = score_range\n",
    "    \n",
    "    print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"\\nHyperparameter Sensitivity:\")\n",
    "    print(f\"  Variance: {score_variance:.6f}\")\n",
    "    print(f\"  Std Dev: {score_std:.6f}\")\n",
    "    print(f\"  Range: {score_range:.4f}\")\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ GridSearchCV completed for all classifiers\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ae229",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Sensitivity Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b09f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "sensitivity_summary = pd.DataFrame({\n",
    "    'Classifier': list(grid_results.keys()),\n",
    "    'Best CV Score': [grid_results[name]['best_score'] for name in grid_results.keys()],\n",
    "    'Test Accuracy': [grid_results[name]['test_accuracy'] for name in grid_results.keys()],\n",
    "    'Test F1': [grid_results[name]['test_f1'] for name in grid_results.keys()],\n",
    "    'Variance': [grid_results[name]['variance'] for name in grid_results.keys()],\n",
    "    'Std Dev': [grid_results[name]['std'] for name in grid_results.keys()],\n",
    "    'Range': [grid_results[name]['range'] for name in grid_results.keys()]\n",
    "})\n",
    "\n",
    "sensitivity_summary = sensitivity_summary.round(4)\n",
    "sensitivity_summary = sensitivity_summary.sort_values('Variance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER SENSITIVITY RANKING - CENSUS INCOME\")\n",
    "print(\"=\"*80)\n",
    "print(sensitivity_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Save results\n",
    "sensitivity_summary.to_csv('outputs/results/census_income_sensitivity.csv', index=False)\n",
    "print(\"\\n✓ Results saved to: outputs/results/census_income_sensitivity.csv\")\n",
    "\n",
    "# Identify most and least sensitive classifiers\n",
    "most_sensitive = sensitivity_summary.iloc[0]['Classifier']\n",
    "least_sensitive = sensitivity_summary.iloc[-1]['Classifier']\n",
    "\n",
    "print(f\"\\nMost Sensitive to Hyperparameters: {most_sensitive}\")\n",
    "print(f\"Least Sensitive to Hyperparameters: {least_sensitive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021a84a",
   "metadata": {},
   "source": [
    "## 5. Visualize Hyperparameter Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Hyperparameter Sensitivity Analysis - Census Income Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Variance comparison\n",
    "ax = axes[0, 0]\n",
    "sensitivity_summary.plot(x='Classifier', y='Variance', kind='bar', ax=ax, color='coral', legend=False)\n",
    "ax.set_title('Hyperparameter Variance by Classifier', fontweight='bold')\n",
    "ax.set_ylabel('Variance')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Range comparison\n",
    "ax = axes[0, 1]\n",
    "sensitivity_summary.plot(x='Classifier', y='Range', kind='bar', ax=ax, color='skyblue', legend=False)\n",
    "ax.set_title('Score Range by Classifier', fontweight='bold')\n",
    "ax.set_ylabel('Range (Max - Min Score)')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Best CV Score vs Test Accuracy\n",
    "ax = axes[0, 2]\n",
    "x_pos = np.arange(len(sensitivity_summary))\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, sensitivity_summary['Best CV Score'], width, label='Best CV Score', color='lightgreen')\n",
    "ax.bar(x_pos + width/2, sensitivity_summary['Test Accuracy'], width, label='Test Accuracy', color='lightcoral')\n",
    "ax.set_title('CV Score vs Test Accuracy', fontweight='bold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(sensitivity_summary['Classifier'], rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Standard Deviation comparison\n",
    "ax = axes[1, 0]\n",
    "sensitivity_summary.plot(x='Classifier', y='Std Dev', kind='bar', ax=ax, color='mediumpurple', legend=False)\n",
    "ax.set_title('Standard Deviation by Classifier', fontweight='bold')\n",
    "ax.set_ylabel('Standard Deviation')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Test F1 Score\n",
    "ax = axes[1, 1]\n",
    "sensitivity_summary.plot(x='Classifier', y='Test F1', kind='bar', ax=ax, color='gold', legend=False)\n",
    "ax.set_title('Test F1-Score by Classifier', fontweight='bold')\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Sensitivity vs Performance scatter\n",
    "ax = axes[1, 2]\n",
    "scatter = ax.scatter(sensitivity_summary['Variance'], sensitivity_summary['Test Accuracy'], \n",
    "                     s=200, c=range(len(sensitivity_summary)), cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "for idx, row in sensitivity_summary.iterrows():\n",
    "    ax.annotate(row['Classifier'], (row['Variance'], row['Test Accuracy']), \n",
    "                fontsize=8, ha='center', va='bottom')\n",
    "ax.set_title('Sensitivity vs Performance', fontweight='bold')\n",
    "ax.set_xlabel('Variance (Sensitivity)')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/census_sensitivity_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualizations saved to: outputs/figures/census_sensitivity_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e508598",
   "metadata": {},
   "source": [
    "## 6. Detailed Hyperparameter Analysis for Each Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a969ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each classifier, visualize how performance varies with each hyperparameter\n",
    "for name in grid_results.keys():\n",
    "    cv_results = grid_results[name]['cv_results']\n",
    "    params = grid_results[name]['best_params']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Detailed Analysis: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create DataFrame from CV results\n",
    "    results_df = pd.DataFrame(cv_results)\n",
    "    \n",
    "    # Display top 10 configurations\n",
    "    print(f\"\\nTop 10 Hyperparameter Configurations:\")\n",
    "    top_10 = results_df.nsmallest(10, 'rank_test_score')[['params', 'mean_test_score', 'std_test_score']]\n",
    "    for idx, row in top_10.iterrows():\n",
    "        print(f\"  Rank {int(results_df.loc[idx, 'rank_test_score'])}: Score={row['mean_test_score']:.4f} (+/-{row['std_test_score']:.4f})\")\n",
    "        print(f\"    Params: {row['params']}\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df.to_csv(f'outputs/gridsearch/census_{name.replace(\" \", \"_\").lower()}_gridsearch.csv', index=False)\n",
    "    print(f\"\\n✓ Detailed results saved to: outputs/gridsearch/census_{name.replace(' ', '_').lower()}_gridsearch.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Detailed hyperparameter analysis complete\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea14e36",
   "metadata": {},
   "source": [
    "## 7. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS - CENSUS INCOME DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. Dataset Characteristics:\")\n",
    "print(f\"   - Samples: {len(df)}\")\n",
    "print(f\"   - Features: {X.shape[1]}\")\n",
    "print(f\"   - Classes: Binary (Income >50K vs <=50K)\")\n",
    "\n",
    "print(f\"\\n2. Best Performing Classifier:\")\n",
    "best_clf = sensitivity_summary.loc[sensitivity_summary['Test Accuracy'].idxmax()]\n",
    "print(f\"   - Classifier: {best_clf['Classifier']}\")\n",
    "print(f\"   - Test Accuracy: {best_clf['Test Accuracy']:.4f}\")\n",
    "print(f\"   - Test F1: {best_clf['Test F1']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. Hyperparameter Sensitivity:\")\n",
    "print(f\"   - Most Sensitive: {sensitivity_summary.iloc[0]['Classifier']} (Variance: {sensitivity_summary.iloc[0]['Variance']:.6f})\")\n",
    "print(f\"   - Least Sensitive: {sensitivity_summary.iloc[-1]['Classifier']} (Variance: {sensitivity_summary.iloc[-1]['Variance']:.6f})\")\n",
    "print(f\"   - Sensitivity Ratio: {sensitivity_summary.iloc[0]['Variance'] / sensitivity_summary.iloc[-1]['Variance']:.2f}x\")\n",
    "\n",
    "print(f\"\\n4. Performance Ranges:\")\n",
    "for idx, row in sensitivity_summary.iterrows():\n",
    "    print(f\"   - {row['Classifier']}: {row['Range']:.4f} (Max-Min score difference)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Analysis complete for Census Income dataset\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
